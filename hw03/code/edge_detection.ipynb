{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3-3VmiUfRNU"
      },
      "source": [
        "# HW3 - Q6: Exploring Inductive Bias of Convolutional Neural Networks and Systematic Experimentation in Machine Learning\n",
        "\n",
        "In this homework, we will study 1) what is inductive bias and how it affects the learning process, and 2) how to conduct systematic experiments in machine learning. We will compare convolutional neural networks (CNNs) and multi-layer perceptrons (MLPs) extensively as an example to study these two topics.\n",
        "\n",
        "## 1. Inductive Bias\n",
        "\n",
        "What is inductive bias? It is the assumption that the learning algorithm makes about the problem domain. Suppose that we build a machine learning system. We want to leverage the specific knowledge about the problem domain to make the learning process **more efficient** and the system **generalize much better** with fewer parameters. Let's be more precise. What do exactly **more efficient** and **generalize much better** mean? The learning process is more efficient 1) if we can learn the model with fewer parameters, 2) if we can learn the model with fewer data, and 3) if we can learn the model with fewer iterations. And the system generalizes much better if the model can generalize to the unseen data well.\n",
        "\n",
        "We have already observed the power of inductive bias. We know that CNN generalizes better than MLP even with the same number of parameters. We partially concluded that is because CNN has the inductive bias that the model is translation invariant. We will study the inductive bias of CNN in more detail in this homework.\n",
        "\n",
        "In this homework, we will use the edge detection task as an example to study the inductive bias of CNN. We will compare CNN and MLP extensively. And we will see when CNN can fail.\n",
        "\n",
        "## 2. Systematic Experimentation in Machine Learning\n",
        "\n",
        "How can we prove our hypothesis that CNN has the inductive bias that the model is translation invariant? We conduct extensive experiments in machine learning research (and other fields) to prove our hypothesis. In this context, systematic experimentation refers to running a series of experiments to prove our hypothesis. In this homework, we will study how to conduct systematic experimentation in machine learning.\n",
        "\n",
        "Let's take a step back and think about 1) what our hypothesis is and 2) what experiments are needed to conduct to prove our hypothesis. The first question is easy. The hypothesis is that CNN has the inductive biases of locality and translational invariance. It is not enough to show that CNN performs better than MLP with the same number of parameters. Then, how do we design the experiments to prove our hypothesis? In this homework, we will design the experiments, conduct the experiments, analyze the results, and draw a conclusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kARHLLg8fRNX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\22020\\AppData\\Local\\Temp\\ipykernel_20820\\3989156832.py:15: DeprecationWarning: Please use `rotate` from the `scipy.ndimage` namespace, the `scipy.ndimage.interpolation` namespace is deprecated.\n",
            "  from scipy.ndimage.interpolation import rotate\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import ToPILImage\n",
        "\n",
        "from PIL import Image\n",
        "from scipy.ndimage.interpolation import rotate\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTjhkOx9jI-z"
      },
      "source": [
        "### Helper functions\n",
        "\n",
        "The following code cell defines function and classes that will be used in the succeeding codes. Feel free to check it if you are not sure about details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P3nybaBIfuel"
      },
      "outputs": [],
      "source": [
        "class EdgeDetectionDataset(Dataset):\n",
        "    def __init__(self, domain_config, mode=\"train\", transform=None) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            domain_config (dict): Domain configuration\n",
        "                data_per_class (int): Number of data per class\n",
        "                num_classes (int): Number of classes\n",
        "                class_type (list): List of class types\n",
        "                spatial_resolution (int): length of height and width of the image\n",
        "                max_edge_width (int): Maximum edge width\n",
        "                max_edge_intensity (float): Maximum edge intensity\n",
        "                min_edge_intensity (float): Minimum edge intensity\n",
        "                max_background_intensity (float): Maximum background intensity\n",
        "                min_background_intensity (float): Minimum background intensity\n",
        "                possible_edge_location_ratio (float): Confine the possible edge location to a ratio of the spatial resolution\n",
        "                num_horizontal_edge (int): Number of horizontal edges\n",
        "                num_vertical_edge (int): Number of vertical edges\n",
        "                use_permutation (bool): Whether to apply random permutation on the image\n",
        "            mode (str): Mode of the dataset (train, val, test)\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.data_per_class = domain_config.get(\"data_per_class\", 1000)\n",
        "        self.num_classes = domain_config.get(\"num_classes\", 3)\n",
        "        self.class_type = domain_config.get(\n",
        "            \"class_type\", [\"horizontal\", \"vertical\", \"none\"]\n",
        "        )\n",
        "        self.spatial_resolution = domain_config.get(\"spatial_resolution\", 28)\n",
        "        self.min_edge_width = domain_config.get(\"min_edge_width\", 1)\n",
        "        self.max_edge_width = domain_config.get(\"max_edge_width\", 4)\n",
        "        self.max_edge_intensity = domain_config.get(\"max_edge_intensity\", 1)\n",
        "        self.min_edge_intensity = domain_config.get(\"min_edge_intensity\", 0.25)\n",
        "        self.max_background_intensity = domain_config.get(\n",
        "            \"max_background_intensity\", 0.2\n",
        "        )\n",
        "        self.min_background_intensity = domain_config.get(\"min_background_intensity\", 0)\n",
        "        self.possible_edge_location_ratio = domain_config.get(\n",
        "            \"possible_edge_location_ratio\", 1.0\n",
        "        )\n",
        "        self.num_horizontal_edge = domain_config.get(\"num_horizontal_edge\", 1)\n",
        "        self.num_vertical_edge = domain_config.get(\"num_vertical_edge\", 1)\n",
        "        self.num_diagonal_edge = domain_config.get(\"num_diagonal_edge\", 1)\n",
        "        self.use_permutation = domain_config.get(\"use_permutation\", False)\n",
        "        self.permutater = domain_config.get(\"permutater\", None)\n",
        "        self.unpermutater = domain_config.get(\"unpermutater\", None)\n",
        "\n",
        "        if self.possible_edge_location_ratio < 1.0:\n",
        "            self.train_val_domain_shift = True\n",
        "        else:\n",
        "            self.train_val_domain_shift = False\n",
        "\n",
        "        self.possible_edge_location = int(\n",
        "            self.possible_edge_location_ratio * self.spatial_resolution\n",
        "        )\n",
        "        self.mode = mode\n",
        "\n",
        "        assert self.num_classes == len(\n",
        "            self.class_type\n",
        "        ), \"Number of classes must match the number of class types\"\n",
        "\n",
        "        assert self.mode in (\n",
        "            \"train\",\n",
        "            \"valid\",\n",
        "            \"test\",\n",
        "        ), \"Mode must be either train, valid, or test\"\n",
        "\n",
        "        self.X = None\n",
        "        self.y = None\n",
        "\n",
        "        if self.use_permutation:\n",
        "            assert self.permutater is not None, \"permutater must be provided\"\n",
        "            assert self.unpermutater is not None, \"Unpermutater must be provided\"\n",
        "\n",
        "        self._generate_dataset()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            int: Length of the dataset\n",
        "        \"\"\"\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            idx (int): Index of the sample\n",
        "        Returns:\n",
        "            tuple: (sample, label)\n",
        "        \"\"\"\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        sample = self.X[idx]\n",
        "        label = self.y[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, label\n",
        "\n",
        "    def get_permutater(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            np.ndarray: Permutation matrix\n",
        "        \"\"\"\n",
        "        return self.permutater\n",
        "\n",
        "    def get_unpermutater(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            np.ndarray: Unpermutation matrix\n",
        "        \"\"\"\n",
        "        return self.unpermutater\n",
        "\n",
        "    def _permute_pixels(self, X):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            X (np.ndarray): Image\n",
        "        Returns:\n",
        "            np.ndarray: Permuted image\n",
        "        \"\"\"\n",
        "        assert X.shape[0] == self.data_per_class, \"Invalid image shape\"\n",
        "        assert len(X.shape) == 4, \"Invalid image shape\"\n",
        "\n",
        "        n, h, w, c = X.shape\n",
        "\n",
        "        X = X.reshape(n, h * w, c)\n",
        "        X = X[:, self.permutater, :]\n",
        "        X = X.reshape(n, h, w, c)\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _edge_intensity(self, edge_type=\"horizontal\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            edge_type (str): Type of edge (horizontal, vertical, both, diagonal)\n",
        "        Returns:\n",
        "            np.ndarray: Edge intensity\n",
        "        \"\"\"\n",
        "        if edge_type == \"horizontal\":\n",
        "            num_edge = self.num_horizontal_edge\n",
        "        elif edge_type == \"vertical\":\n",
        "            num_edge = self.num_vertical_edge\n",
        "        elif edge_type == \"diagonal\":\n",
        "            num_edge = self.num_diagonal_edge\n",
        "        elif edge_type == \"both\":\n",
        "            num_edge = self.num_horizontal_edge + self.num_vertical_edge\n",
        "        else:\n",
        "            raise ValueError(\"Invalid edge type\")\n",
        "\n",
        "        return np.random.uniform(\n",
        "            self.min_edge_intensity,\n",
        "            self.max_edge_intensity,\n",
        "            size=(self.data_per_class, num_edge),\n",
        "        )\n",
        "\n",
        "    def _edge_location(self, edge_type=\"horizontal\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            edge_type (str): Type of edge (horizontal, vertical, both, diagonal)\n",
        "        Returns:\n",
        "            np.ndarray: Edge location\n",
        "        \"\"\"\n",
        "        max_edge_width = self.max_edge_width + 1\n",
        "        if edge_type == \"horizontal\":\n",
        "            num_edge = self.num_horizontal_edge\n",
        "        elif edge_type == \"vertical\":\n",
        "            num_edge = self.num_vertical_edge\n",
        "        elif edge_type == \"diagonal\":\n",
        "            num_edge = self.num_diagonal_edge\n",
        "            max_edge_width = int(self.max_edge_width / np.sqrt(2))\n",
        "        elif edge_type == \"both\":\n",
        "            num_edge = self.num_horizontal_edge + self.num_vertical_edge\n",
        "        else:\n",
        "            raise ValueError(\"Invalid edge type\")\n",
        "\n",
        "        edge_width = np.random.randint(\n",
        "            self.min_edge_width, max_edge_width, size=(self.data_per_class, num_edge)\n",
        "        )\n",
        "\n",
        "        if self.mode == \"train\" and self.train_val_domain_shift:\n",
        "            edge_location_start_idx = np.random.randint(\n",
        "                0,\n",
        "                self.possible_edge_location - edge_width,\n",
        "                size=(self.data_per_class, num_edge),\n",
        "            )\n",
        "            edge_location_end_idx = np.clip(\n",
        "                edge_location_start_idx + edge_width,\n",
        "                0,\n",
        "                self.possible_edge_location-1,\n",
        "            )\n",
        "\n",
        "        elif self.mode == \"valid\" and self.train_val_domain_shift:\n",
        "            edge_location_start_idx = np.random.randint(\n",
        "                self.possible_edge_location,\n",
        "                self.spatial_resolution - edge_width,\n",
        "                size=(self.data_per_class, num_edge),\n",
        "            )\n",
        "            edge_location_end_idx = np.clip(\n",
        "                edge_location_start_idx + edge_width,\n",
        "                self.possible_edge_location,\n",
        "                self.spatial_resolution-1,\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            edge_location_start_idx = np.random.randint(\n",
        "                0,\n",
        "                self.spatial_resolution - edge_width,\n",
        "                size=(self.data_per_class, num_edge),\n",
        "            )\n",
        "            edge_location_end_idx = np.clip(\n",
        "                edge_location_start_idx + edge_width,\n",
        "                0,\n",
        "                self.spatial_resolution-1,\n",
        "            )\n",
        "\n",
        "        return edge_location_start_idx, edge_location_end_idx\n",
        "\n",
        "    def _generate_hoizontal_edge_images(self):\n",
        "        \"\"\"\n",
        "        Generate horizontal edge images\n",
        "        Returns:\n",
        "            np.ndarray: Generated horizontal edge images\n",
        "        \"\"\"\n",
        "        assert (\n",
        "            self.num_horizontal_edge > 0\n",
        "        ), \"Number of horizontal edge must be greater than 0\"\n",
        "\n",
        "        X = self._generate_background_images()\n",
        "\n",
        "        edge_location_start_idx, edge_location_end_idx = self._edge_location(\n",
        "            edge_type=\"horizontal\"\n",
        "        )\n",
        "        edge_intensity = self._edge_intensity()\n",
        "\n",
        "        for i in range(self.data_per_class):\n",
        "            for j in range(self.num_horizontal_edge):\n",
        "                X[\n",
        "                    i, edge_location_start_idx[i, j] : edge_location_end_idx[i, j], :, :\n",
        "                ] = edge_intensity[i, j]\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _generate_vertical_edge_images(self):\n",
        "        \"\"\"\n",
        "        Generate vertical edge images\n",
        "        Returns:\n",
        "            np.ndarray: Generated vertical edge images\n",
        "        \"\"\"\n",
        "        assert (\n",
        "            self.num_vertical_edge > 0\n",
        "        ), \"Number of vertical edge must be greater than 0\"\n",
        "\n",
        "        X = self._generate_background_images()\n",
        "\n",
        "        edge_location_start_idx, edge_location_end_idx = self._edge_location(\n",
        "            edge_type=\"vertical\"\n",
        "        )\n",
        "        edge_intensity = self._edge_intensity()\n",
        "\n",
        "        for i in range(self.data_per_class):\n",
        "            for j in range(self.num_vertical_edge):\n",
        "                X[\n",
        "                    i,\n",
        "                    :,\n",
        "                    edge_location_start_idx[i, j] : edge_location_end_idx[i, j],\n",
        "                    :,\n",
        "                ] = edge_intensity[i, j]\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _generate_both_edge_images(self):\n",
        "        \"\"\"\n",
        "        Generate horizontal/vertical edge images\n",
        "        Returns:\n",
        "            np.ndarray: Generated horizontal/vertical edge images\n",
        "        \"\"\"\n",
        "        assert (\n",
        "            self.num_horizontal_edge > 0\n",
        "        ), \"Number of horizontal edge must be greater than 0\"\n",
        "        assert (\n",
        "            self.num_vertical_edge > 0\n",
        "        ), \"Number of vertical edge must be greater than 0\"\n",
        "\n",
        "        X = self._generate_background_images()\n",
        "\n",
        "        edge_location_start_idx, edge_location_end_idx = self._edge_location(\n",
        "            edge_type=\"both\"\n",
        "        )\n",
        "        edge_intensity = self._edge_intensity(edge_type=\"both\")\n",
        "\n",
        "        for i in range(self.data_per_class):\n",
        "            for j in range(self.num_horizontal_edge):\n",
        "                X[\n",
        "                    i,\n",
        "                    edge_location_start_idx[i, j] : edge_location_end_idx[i, j],\n",
        "                    :,\n",
        "                    :,\n",
        "                ] = edge_intensity[i, j]\n",
        "            for j in range(self.num_vertical_edge):\n",
        "                X[\n",
        "                    i,\n",
        "                    :,\n",
        "                    edge_location_start_idx[i, j] : edge_location_end_idx[i, j],\n",
        "                ] = edge_intensity[i, self.num_horizontal_edge + j]\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _generate_diagonal_edge_images(self):\n",
        "        \"\"\"\n",
        "        Generate diagonal edge images by rotating images\n",
        "        Returns:\n",
        "            np.ndarray: Generated diagonal edge images\n",
        "        \"\"\"\n",
        "        assert (\n",
        "            self.num_diagonal_edge > 0\n",
        "        ), \"Number of diagonal edge must be greater than 0\"\n",
        "\n",
        "        X = self._generate_background_images()\n",
        "        background_intensity = np.mean(X, axis=(1, 2, 3))\n",
        "\n",
        "        edge_location_start_idx, edge_location_end_idx = self._edge_location(\n",
        "            edge_type=\"diagonal\"\n",
        "        )\n",
        "        edge_intensity = self._edge_intensity(edge_type=\"diagonal\")\n",
        "\n",
        "        random_angle = np.random.choice(\n",
        "            [30, 45, 120, 135], size=(self.data_per_class, self.num_diagonal_edge)\n",
        "        )\n",
        "\n",
        "        for i in range(self.data_per_class):\n",
        "            for j in range(self.num_diagonal_edge):\n",
        "                if i % 2 == 0:  # horizontal\n",
        "                    X[\n",
        "                        i,\n",
        "                        edge_location_start_idx[i, j] : edge_location_end_idx[i, j],\n",
        "                        :,\n",
        "                        :,\n",
        "                    ] = edge_intensity[i, j]\n",
        "                else:  # vertical\n",
        "                    X[\n",
        "                        i,\n",
        "                        :,\n",
        "                        edge_location_start_idx[i, j] : edge_location_end_idx[i, j],\n",
        "                    ] = edge_intensity[i, j]\n",
        "                X[i] = rotate(\n",
        "                    X[i],\n",
        "                    random_angle[i, j],\n",
        "                    reshape=False,\n",
        "                    mode=\"constant\",\n",
        "                    cval=background_intensity[i],\n",
        "                )\n",
        "        return X\n",
        "\n",
        "    def _generate_background_images(self):\n",
        "        \"\"\"\n",
        "        Generate background images\n",
        "        Returns:\n",
        "            np.ndarray: Generated background images\n",
        "        \"\"\"\n",
        "        X = np.ones(\n",
        "            (self.data_per_class, self.spatial_resolution, self.spatial_resolution, 1),\n",
        "        )  # NHWC format\n",
        "        X *= np.random.uniform(\n",
        "            self.min_background_intensity,\n",
        "            self.max_background_intensity,\n",
        "            size=(self.data_per_class, 1, 1, 1),\n",
        "        )\n",
        "        return X\n",
        "\n",
        "    def get_image_statistics(self):\n",
        "        \"\"\"\n",
        "        Get image statistics\n",
        "        Returns:\n",
        "            tuple: (mean, std)\n",
        "            mean (float): Mean of the images\n",
        "            std (float): Standard deviation of the images\n",
        "        \"\"\"\n",
        "        return self._mean, self._std\n",
        "\n",
        "    def _generate_dataset(self):\n",
        "        \"\"\"\n",
        "        Generate dataset\n",
        "        Returns:\n",
        "            tuple: (X, y)\n",
        "            X (list of PIL Image): Generated images\n",
        "            y (np.ndarray): Generated labels\n",
        "        \"\"\"\n",
        "        num_data = self.data_per_class * self.num_classes\n",
        "        self.X = np.zeros(\n",
        "            (num_data, self.spatial_resolution, self.spatial_resolution, 1)\n",
        "        )\n",
        "        self.y = np.zeros((num_data,), dtype=np.int64)\n",
        "        for i in range(self.num_classes):\n",
        "            class_type = self.class_type[i]\n",
        "            if class_type == \"horizontal\":\n",
        "                X = self._generate_hoizontal_edge_images()\n",
        "            elif class_type == \"vertical\":\n",
        "                X = self._generate_vertical_edge_images()\n",
        "            elif class_type == \"both\":\n",
        "                X = self._generate_both_edge_images()\n",
        "            elif class_type == \"diagonal\":\n",
        "                X = self._generate_diagonal_edge_images()\n",
        "            elif class_type == \"none\":\n",
        "                X = self._generate_background_images()\n",
        "            else:\n",
        "                raise ValueError(\"Invalid class type\")\n",
        "\n",
        "            assert X.shape == (\n",
        "                self.data_per_class,\n",
        "                self.spatial_resolution,\n",
        "                self.spatial_resolution,\n",
        "                1,\n",
        "            )  # NHWC format\n",
        "\n",
        "            # permute pixels\n",
        "            if self.use_permutation:\n",
        "                X = self._permute_pixels(X)\n",
        "\n",
        "            self.X[i * self.data_per_class : (i + 1) * self.data_per_class] = X\n",
        "            self.y[i * self.data_per_class : (i + 1) * self.data_per_class] = i\n",
        "\n",
        "        # Compute mean and std\n",
        "        self._mean = np.mean(self.X)\n",
        "        self._std = np.std(self.X)\n",
        "\n",
        "        # np.float32 -> np.uint8\n",
        "        self.X = (self.X * 255).astype(np.uint8)\n",
        "\n",
        "        # Convert ndarray to PIL Image\n",
        "        self.X = [T.functional.to_pil_image(x) for x in self.X]\n",
        "\n",
        "\n",
        "def count_parameters(model, only_trainable=False):\n",
        "    if only_trainable:\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    else:\n",
        "        return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def freeze_conv_layer(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        if name.startswith('conv'):\n",
        "            param.requires_grad = False\n",
        "\n",
        "def init_conv_kernel_with_edge_detector(model):\n",
        "    # Get kernel size\n",
        "    kernel_size = model.conv1.kernel_size[0]\n",
        "\n",
        "    # number of filters should be 3\n",
        "    num_filters = model.conv1.out_channels\n",
        "    assert num_filters == 3, \"Number of filters should be 3\"\n",
        "\n",
        "    if kernel_size == 2:\n",
        "        # 2 x 2 edge detector\n",
        "        horizontal_edge_detector = torch.tensor([[1, 1], [-1, -1]], dtype=torch.float32)\n",
        "        vertical_edge_detector = torch.tensor([[1, -1], [1, -1]], dtype=torch.float32)\n",
        "        none_edge_detector = torch.tensor([[0, 0], [0, 0]], dtype=torch.float32)\n",
        "    elif kernel_size == 7:\n",
        "        # 7 x 7 edge detector\n",
        "        horizontal_edge_detector = torch.tensor([\n",
        "            [0, 0, 0, 0, 0, 0, 0],\n",
        "            [0, 0, 0, 0, 0, 0, 0],\n",
        "            [1, 1, 1, 1, 1, 1, 1],\n",
        "            [-1, -1, -1, -1, -1, -1, -1],\n",
        "            [0, 0, 0, 0, 0, 0, 0],\n",
        "            [0, 0, 0, 0, 0, 0, 0],\n",
        "            [0, 0, 0, 0, 0, 0, 0]], dtype=torch.float32)\n",
        "        vertical_edge_detector = horizontal_edge_detector.T\n",
        "        none_edge_detector = model.conv1.weight.data[0, 0] # torch.zeros((7, 7), dtype=torch.float32)\n",
        "    else:\n",
        "        horizontal_edge_detector = torch.from_numpy(custom_sobel((kernel_size, kernel_size), 0))\n",
        "        vertical_edge_detector = torch.from_numpy(custom_sobel((kernel_size, kernel_size), 1))\n",
        "        none_edge_detector = torch.from_numpy(np.zeros((kernel_size, kernel_size)))\n",
        "\n",
        "    edge_detector = torch.stack([horizontal_edge_detector, vertical_edge_detector, none_edge_detector])\n",
        "    model.conv1.weight.data = edge_detector.view(model.num_filter, 1, model.kernel_size, model.kernel_size)\n",
        "    model.conv2.weight.data = torch.cat([model.conv1.weight.data, model.conv1.weight.data, model.conv1.weight.data], dim=1)\n",
        "\n",
        "    # type casting\n",
        "    model.conv1.weight.data = model.conv1.weight.data.type(torch.FloatTensor)\n",
        "    model.conv2.weight.data = model.conv2.weight.data.type(torch.FloatTensor)\n",
        "\n",
        "    # bias\n",
        "    model.conv1.bias.data = torch.tensor([0, 0, 0], dtype=torch.float32)\n",
        "    model.conv2.bias.data = torch.tensor([0, 0, 0], dtype=torch.float32)\n",
        "\n",
        "def custom_sobel(shape, axis):\n",
        "    \"\"\"\n",
        "    shape must be odd: eg. (5,5)\n",
        "    axis is the direction, with 0 to positive x and 1 to positive y\n",
        "    \"\"\"\n",
        "    k = np.zeros(shape, dtype=np.float32)\n",
        "    p = [(j,i) for j in range(shape[0])\n",
        "           for i in range(shape[1])\n",
        "           if not (i == (shape[1] -1)/2. and j == (shape[0] -1)/2.)]\n",
        "\n",
        "    for j, i in p:\n",
        "        j_ = int(j - (shape[0] -1)/2.)\n",
        "        i_ = int(i - (shape[1] -1)/2.)\n",
        "        k[j,i] = (i_ if axis==0 else j_)/float(i_*i_ + j_*j_)\n",
        "    return k\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Set the seed for all random number generators.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    train_loader,\n",
        "    device,\n",
        "    epoch,\n",
        "    log_interval=100,\n",
        "    verbose=True,\n",
        "):\n",
        "    model.train()\n",
        "    # return the average loss and accuracy\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        pred = output.argmax(\n",
        "            dim=1, keepdim=True\n",
        "        )\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        if batch_idx % log_interval == 0 and verbose:\n",
        "            print(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_accuracy = correct / len(train_loader.dataset) * 100.\n",
        "\n",
        "    return train_loss, train_accuracy\n",
        "\n",
        "def _generate_confusion_matrix(pred_list, target_list):\n",
        "    pred_list = torch.cat(pred_list)\n",
        "    target_list = torch.cat(target_list)\n",
        "\n",
        "    assert pred_list.shape[0] == target_list.shape[0], \"predictions and targets should have the same length\"\n",
        "\n",
        "    matrix_size = max(max(pred_list), max(target_list)) + 1\n",
        "    confusion_matrix = torch.zeros(matrix_size, matrix_size)\n",
        "\n",
        "    for t, p in zip(target_list.view(-1), pred_list.view(-1)):\n",
        "        confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "    return confusion_matrix.cpu().numpy()\n",
        "\n",
        "def evaluate(model, criterion, valid_loader, device, verbose=True):\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    pred_list, target_list = [], []\n",
        "    confusion_matrix = torch.zeros(4, 4)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in valid_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            valid_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(\n",
        "                dim=1, keepdim=True\n",
        "            )  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "            pred_list.append(pred)\n",
        "            target_list.append(target)\n",
        "\n",
        "    confusion_matrix = _generate_confusion_matrix(pred_list, target_list)\n",
        "\n",
        "    valid_loss /= len(valid_loader.dataset)\n",
        "    valid_accuracy = 100.0 * correct / len(valid_loader.dataset)\n",
        "\n",
        "    if verbose:\n",
        "        print(\n",
        "            \"Validation Result: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n",
        "                valid_loss, correct, len(valid_loader.dataset), valid_accuracy\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return valid_loss, valid_accuracy, confusion_matrix\n",
        "\n",
        "\n",
        "def vis_training_curve(cnn_train_loss, cnn_train_acc, mlp_train_loss, mlp_train_acc, label=\"MLP\"):\n",
        "    # if mlp lists are empty, then we are only plotting the CNN\n",
        "    if mlp_train_loss is None or len(mlp_train_loss) == 0:\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
        "        ax[0].plot(cnn_train_loss, label=\"CNN\")\n",
        "        ax[0].set_title(\"Training Loss\")\n",
        "        ax[0].set_xlabel(\"Epoch\")\n",
        "        ax[0].set_ylabel(\"Loss\")\n",
        "        ax[0].legend()\n",
        "        ax[0].grid()\n",
        "\n",
        "        ax[1].plot(cnn_train_acc, label=\"CNN\")\n",
        "        ax[1].set_title(\"Training Accuracy\")\n",
        "        ax[1].set_xlabel(\"Epoch\")\n",
        "        ax[1].set_ylabel(\"Accuracy (%)\")\n",
        "        ax[1].legend()\n",
        "        ax[1].grid()\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    # if cnn lists are empty, then we are only plotting the MLP\n",
        "    elif cnn_train_loss is None or len(cnn_train_loss) == 0:\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
        "        ax[0].plot(mlp_train_loss, label=label)\n",
        "        ax[0].set_title(\"Training Loss\")\n",
        "        ax[0].set_xlabel(\"Epoch\")\n",
        "        ax[0].set_ylabel(\"Loss\")\n",
        "        ax[0].legend()\n",
        "        ax[0].grid()\n",
        "\n",
        "        ax[1].plot(mlp_train_acc, label=label)\n",
        "        ax[1].set_title(\"Training Accuracy\")\n",
        "        ax[1].set_xlabel(\"Epoch\")\n",
        "        ax[1].set_ylabel(\"Accuracy (%)\")\n",
        "        ax[1].legend()\n",
        "        ax[1].grid()\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    # if both lists are not empty, then we are plotting both CNN and MLP\n",
        "    else:\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
        "        ax[0].plot(cnn_train_loss, label=\"CNN\")\n",
        "        ax[0].plot(mlp_train_loss, label=label)\n",
        "        ax[0].set_title(\"Training Loss\")\n",
        "        ax[0].set_xlabel(\"Epoch\")\n",
        "        ax[0].set_ylabel(\"Loss\")\n",
        "        ax[0].legend()\n",
        "        ax[0].grid()\n",
        "\n",
        "        ax[1].plot(cnn_train_acc, label=\"CNN\")\n",
        "        ax[1].plot(mlp_train_acc, label=label)\n",
        "        ax[1].set_title(\"Training Accuracy\")\n",
        "        ax[1].set_xlabel(\"Epoch\")\n",
        "        ax[1].set_ylabel(\"Accuracy (%)\")\n",
        "        ax[1].legend()\n",
        "        ax[1].grid()\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def vis_validation_curve(cnn_valid_loss, cnn_valid_acc, mlp_valid_loss, mlp_valid_acc,  label=\"MLP\"):\n",
        "    # if mlp lists are empty, then we are only plotting the CNN\n",
        "    if mlp_valid_loss is None or len(mlp_valid_loss) == 0:\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
        "        ax[0].plot(cnn_valid_loss, label=\"CNN\")\n",
        "        ax[0].set_title(\"Validation Loss\")\n",
        "        ax[0].set_xlabel(\"Epoch\")\n",
        "        ax[0].set_ylabel(\"Loss\")\n",
        "        ax[0].legend()\n",
        "        ax[0].grid()\n",
        "\n",
        "        ax[1].plot(cnn_valid_acc, label=\"CNN\")\n",
        "        ax[1].set_title(\"Validation Accuracy\")\n",
        "        ax[1].set_xlabel(\"Epoch\")\n",
        "        ax[1].set_ylabel(\"Accuracy (%)\")\n",
        "        ax[1].legend()\n",
        "        ax[1].grid()\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    # if cnn lists are empty, then we are only plotting the MLP\n",
        "    elif cnn_valid_loss is None or len(cnn_valid_loss) == 0:\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
        "        ax[0].plot(mlp_valid_loss, label=label)\n",
        "        ax[0].set_title(\"Validation Loss\")\n",
        "        ax[0].set_xlabel(\"Epoch\")\n",
        "        ax[0].set_ylabel(\"Loss\")\n",
        "        ax[0].legend()\n",
        "        ax[0].grid()\n",
        "\n",
        "        ax[1].plot(mlp_valid_acc, label=label)\n",
        "        ax[1].set_title(\"Validation Accuracy\")\n",
        "        ax[1].set_xlabel(\"Epoch\")\n",
        "        ax[1].set_ylabel(\"Accuracy (%)\")\n",
        "        ax[1].legend()\n",
        "        ax[1].grid()\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    # if both lists are not empty, then we are plotting both CNN and MLP\n",
        "    else:\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
        "        ax[0].plot(cnn_valid_loss, label=\"CNN\")\n",
        "        ax[0].plot(mlp_valid_loss, label=label)\n",
        "        ax[0].set_title(\"Validation Loss\")\n",
        "        ax[0].set_xlabel(\"Epoch\")\n",
        "        ax[0].set_ylabel(\"Loss\")\n",
        "        ax[0].legend()\n",
        "        ax[0].grid()\n",
        "\n",
        "        ax[1].plot(cnn_valid_acc, label=\"CNN\")\n",
        "        ax[1].plot(mlp_valid_acc, label=label)\n",
        "        ax[1].set_title(\"Validation Accuracy\")\n",
        "        ax[1].set_xlabel(\"Epoch\")\n",
        "        ax[1].set_ylabel(\"Accuracy (%)\")\n",
        "        ax[1].legend()\n",
        "        ax[1].grid()\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def vis_kernel(tensor, ch=0, allkernels=False, nrow=8, padding=1, title=None, cmap=\"Blues\", ax=None):\n",
        "    n, c, h, w = tensor.shape\n",
        "\n",
        "    if allkernels:\n",
        "        tensor = tensor.view(n * c, -1, h, w)\n",
        "    elif c != 3:\n",
        "        tensor = tensor[:, ch, :, :].unsqueeze(dim=1)\n",
        "\n",
        "    rows = np.min((tensor.shape[0] // nrow + 1, 64))\n",
        "    grid = (\n",
        "        torchvision.utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
        "        .numpy()\n",
        "        .transpose((1, 2, 0))\n",
        "    )\n",
        "    plt.figure(figsize=(nrow, rows))\n",
        "    plt.imshow(grid, cmap=cmap)\n",
        "    # plt.colorbar(cmap=cmap)\n",
        "    if title is not None:\n",
        "        plt.title(title, fontsize=\"small\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.ioff()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def vis_confusion_matrix(confusion_matrix, class_names=None, title=None):\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(confusion_matrix, cmap=plt.cm.Blues)\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    matrix_size = confusion_matrix.shape[0]\n",
        "\n",
        "    if class_names is not None:\n",
        "        assert len(class_names) == matrix_size, \"Class names must be same length as confusion matrix\"\n",
        "        ax.set_xticklabels([\"\"] + class_names, rotation=90)\n",
        "        ax.set_yticklabels([\"\"] + class_names)\n",
        "\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"True\")\n",
        "    ax.xaxis.set_label_position(\"top\")\n",
        "    ax.xaxis.tick_top()\n",
        "    ax.set_title(title)\n",
        "\n",
        "    for (i, j), z in np.ndenumerate(confusion_matrix):\n",
        "        ax.text(j, i, \"{:0.1f}\".format(z), ha=\"center\", va=\"center\")\n",
        "\n",
        "\n",
        "def vis_unpermuted_dataset(dataset, num_classes, num_show_per_class, unpermutator):\n",
        "    f, axarr = plt.subplots(num_classes, num_show_per_class, figsize=(20, 2*num_classes))\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        for j in range(num_show_per_class):\n",
        "            img = dataset[i * num_show_per_class + j][0]\n",
        "            label = dataset[i * num_show_per_class + j][1]\n",
        "\n",
        "            if isinstance(img, torch.Tensor):\n",
        "                img = img.numpy().transpose((1, 2, 0))\n",
        "                h, w, c = img.shape\n",
        "                assert c == 1\n",
        "                img = img.reshape((h * w, c))\n",
        "                img = img[unpermutator, :]\n",
        "                img = img.reshape(h, w)\n",
        "                axarr[i, j].imshow(img, cmap=\"gray\", vmin=0, vmax=1)\n",
        "\n",
        "            elif isinstance(img, Image.Image):\n",
        "                img = np.array(img)\n",
        "                h, w = img.shape\n",
        "                img = img.reshape(h*w)\n",
        "                img = img[unpermutator]\n",
        "                img = img.reshape(h, w)\n",
        "                img = T.functional.to_pil_image(img)\n",
        "                axarr[i, j].imshow(img, cmap=\"gray\", vmin=0, vmax=255)\n",
        "\n",
        "            axarr[i, j].axis(\"off\")\n",
        "            axarr[i, j].set_title('Class: {}'.format(label))\n",
        "\n",
        "\n",
        "def vis_dataset(dataset, num_classes=3, num_show_per_class=10):\n",
        "    f, axarr = plt.subplots(num_classes, num_show_per_class, figsize=(20, 2*num_classes))\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        for j in range(num_show_per_class):\n",
        "            img = dataset[i * num_show_per_class + j][0]\n",
        "            label = dataset[i * num_show_per_class + j][1]\n",
        "\n",
        "            if isinstance(img, torch.Tensor):\n",
        "                img = img.numpy().transpose((1, 2, 0))\n",
        "                img = img.squeeze()\n",
        "                axarr[i, j].imshow(img, cmap=\"gray\", vmin=0, vmax=1)\n",
        "            elif isinstance(img, Image.Image):\n",
        "                axarr[i, j].imshow(img, cmap=\"gray\", vmin=0, vmax=255)\n",
        "            axarr[i, j].axis(\"off\")\n",
        "            axarr[i, j].set_title('Class: {}'.format(label))\n",
        "\n",
        "\n",
        "class WiderCNN(nn.Module):\n",
        "    def __init__(self, input_channel=1, num_filters=6, kernel_size=7, num_classes=5):\n",
        "        super(WiderCNN, self).__init__()\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.conv1 = nn.Conv2d(input_channel, num_filters, kernel_size=kernel_size, padding=padding, padding_mode='reflect')\n",
        "        self.conv2 = nn.Conv2d(num_filters, num_filters, kernel_size=kernel_size, padding=padding, padding_mode='reflect')\n",
        "        self.maxpool = nn.MaxPool2d(2, 2)\n",
        "        self.fc = nn.Linear(num_filters, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.maxpool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1)).squeeze()\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class DeeperCNN(nn.Module):\n",
        "    def __init__(self, input_channel=1, num_filters=3, kernel_size=7, num_classes=5):\n",
        "        super().__init__()\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.conv1 = nn.Conv2d(input_channel, num_filters, kernel_size=kernel_size, padding=padding, padding_mode='reflect')\n",
        "        self.conv2 = nn.Conv2d(num_filters, num_filters, kernel_size=kernel_size, padding=padding, padding_mode='reflect')\n",
        "        self.conv3 = nn.Conv2d(num_filters, num_filters, kernel_size=kernel_size, padding=padding, padding_mode='reflect')\n",
        "        self.conv4 = nn.Conv2d(num_filters, num_filters, kernel_size=kernel_size, padding=padding, padding_mode='reflect')\n",
        "        self.maxpool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(num_filters, num_classes)\n",
        "\n",
        "        self.num_filters = num_filters\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.maxpool(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1)).squeeze()\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_filters=3, kernel_size=2, num_classes=3):\n",
        "        super().__init__()\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.conv1 = nn.Conv2d(1, num_filters, kernel_size, padding=padding, padding_mode='reflect')\n",
        "        self.conv2 = nn.Conv2d(num_filters, num_filters, kernel_size, padding=padding, padding_mode='reflect')\n",
        "        self.maxpool = nn.MaxPool2d(2, 2)\n",
        "        self.fc = nn.Linear(num_filters, num_classes)\n",
        "\n",
        "        self.num_filter = num_filters\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1)).squeeze()\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_features(self, x):\n",
        "        feat_list = []\n",
        "        x = self.conv1(x)\n",
        "        feat_list.append(x)\n",
        "        x = F.relu(x)\n",
        "        feat_list.append(x)\n",
        "        x = self.maxpool(x)\n",
        "        feat_list.append(x)\n",
        "        x = self.conv2(x)\n",
        "        feat_list.append(x)\n",
        "        x = F.relu(x)\n",
        "        feat_list.append(x)\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1)).squeeze()\n",
        "        feat_list.append(x)\n",
        "\n",
        "        return feat_list\n",
        "\n",
        "class SimpleCNN_avgpool(nn.Module):\n",
        "    def __init__(self, num_filters=3, kernel_size=2, num_classes=3):\n",
        "        super().__init__()\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.conv1 = nn.Conv2d(1, num_filters, kernel_size, padding=padding, padding_mode='reflect')\n",
        "        self.conv2 = nn.Conv2d(num_filters, num_filters, kernel_size, padding=padding, padding_mode='reflect')\n",
        "        self.avgpool = nn.AvgPool2d(2, 2)\n",
        "        self.fc = nn.Linear(num_filters, num_classes)\n",
        "\n",
        "        self.num_filter = num_filters\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1)).squeeze()\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_features(self, x):\n",
        "        feat_list = []\n",
        "        x = self.conv1(x)\n",
        "        feat_list.append(x)\n",
        "        x = F.relu(x)\n",
        "        feat_list.append(x)\n",
        "        x = self.avgpool(x)\n",
        "        feat_list.append(x)\n",
        "        x = self.conv2(x)\n",
        "        feat_list.append(x)\n",
        "        x = F.relu(x)\n",
        "        feat_list.append(x)\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1)).squeeze()\n",
        "        feat_list.append(x)\n",
        "\n",
        "        return feat_list\n",
        "\n",
        "class ThreeLayerCNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim=(1, 28, 28),\n",
        "        num_filters=64, #make it explicit\n",
        "        filter_size=7,\n",
        "        hidden_dim=100,\n",
        "        num_classes=4,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        A three-layer convolutional network with the following architecture:\n",
        "        conv - relu - 2x2 max pool - affine - relu - affine - softmax\n",
        "        The network operates on minibatches of data that have shape (N, C, H, W)\n",
        "        consisting of N images, each with height H and width W and with C input\n",
        "        channels.\n",
        "        Args:\n",
        "            kernel_size (int): Size of the convolutional kernel\n",
        "            channel_size (int): Number of channels in the convolutional layer\n",
        "            linear_layer_input_dim (int): Number of input features to the linear layer\n",
        "            output_dim (int): Number of output features\n",
        "        \"\"\"\n",
        "        super(ThreeLayerCNN, self).__init__()\n",
        "        C, H, W = input_dim\n",
        "\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            C, num_filters, filter_size, stride=1, padding=(filter_size - 1) // 2\n",
        "        )\n",
        "        self.max_pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            num_filters, num_filters * 2, filter_size, padding=(filter_size - 1) // 2\n",
        "        )\n",
        "        self.fc1 = nn.Linear(num_filters * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1)).squeeze()\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TwoLayerMLP(nn.Module):\n",
        "    def __init__(self, input_dim=(1, 28, 28), hidden_dim=10, num_classes=3):\n",
        "        super(TwoLayerMLP, self).__init__()\n",
        "        C, H, W = input_dim\n",
        "        self.fc1 = nn.Linear(C * H * W, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class ThreeLayerMLP(nn.Module):\n",
        "    def __init__(self, input_dim=(1, 28, 28), hidden_dims=[10, 10], num_classes=3, seed=7):\n",
        "        \"\"\"\n",
        "        A three-layer fully-connected neural network with ReLU nonlinearity\n",
        "        \"\"\"\n",
        "        super(ThreeLayerMLP, self).__init__()\n",
        "        torch.manual_seed(seed)\n",
        "        C, H, W = input_dim\n",
        "        self.fc1 = nn.Linear(C * H * W, hidden_dims[0])\n",
        "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
        "        self.fc3 = nn.Linear(hidden_dims[1], num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RK_EKFhwgUBT"
      },
      "outputs": [],
      "source": [
        "seed = 10\n",
        "set_seed(seed)\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAqnKAQofRNZ"
      },
      "source": [
        "## Generate Dataset\n",
        "\n",
        "What would be an excellent dataset to study the inductive bias of CNN? First, have to start with the problem as simple as possible. The complex problem makes it hard to understand the underlying mechanism and is challenging to debug in experimental settings. Hence, we choose the edge detection task as an example to study the inductive bias of CNN. Because\n",
        "1. Edge detection is a straightforward task,\n",
        "2. It is easy to generate the dataset,\n",
        "\n",
        "3. The edge of the image is a very fundamental low-level feature useful to every computer vision task such as object detection and finally,\n",
        "\n",
        "4. Edge detection is an excellent example of studying the inductive bias of CNN.\n",
        "\n",
        "We will generate the dataset for this toy problem. The dataset consists of 10 images of size 28x28 per class, which are all grey scales. Each image contains a vertical edge, a horizontal edge, or nothing. The labels are 0 for vertical edges, 1 for horizontal edges, and 2 for nothing.\n",
        "\n",
        "`EdgeDetectionDataset` class is a dataset class that generates and loads the dataset. The dataset inherits `torch.utils.data.Dataset`, and it generates data when it is initialized. This class takes two arguments: `domain_config` and `transform.` `domain_config` is a dictionary that specifies the domain information of train/valid dataset, such as the number of images per class and the size of the image. `transform` is a function that transforms the image. In this homework, we will use `torchvision.transforms.ToTensor()` to convert the image to a tensor.\n",
        "\n",
        "We highly recommend you read the implementation of `EdgeDetectionDataset` class in `dataset/edge_detection_dataset.py` to understand how the dataset is generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iTVTebk0fRNZ"
      },
      "outputs": [],
      "source": [
        "# Define the domain configuration of the dataset\n",
        "set_seed(seed)\n",
        "\n",
        "visualize_data_config = dict(\n",
        "    data_per_class=10,\n",
        "    num_classes=3,\n",
        "    class_type=[\"horizontal\", \"vertical\", \"none\"],\n",
        ")\n",
        "\n",
        "visualize_dataset = EdgeDetectionDataset(visualize_data_config, mode='train', transform=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUrS-FAdfRNZ"
      },
      "source": [
        "## Visualize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BnW_atsqfRNa"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2000x600 with 30 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "vis_dataset(visualize_dataset, num_classes=3, num_show_per_class=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzFdaJUXfRNa"
      },
      "source": [
        "## Q1. Overfitting Models to Small Dataset\n",
        "\n",
        "In this problem, we will make our models overfit the small dataset to test the model architecture and our synthetic dataset. We use the same dataset for both models. Let's generate a small dataset with ten images per class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pnzVT0_4fRNb"
      },
      "outputs": [],
      "source": [
        "set_seed(seed)\n",
        "\n",
        "small_dataset_config = None\n",
        "small_dataset = None\n",
        "transforms = T.Compose([T.ToTensor()])\n",
        "\n",
        "#############################################################################\n",
        "# TODO: Generate dataset with 10 images per class                           #\n",
        "# Hint: Refer visualize_data_config and use EdgeDetectionDataset and        #\n",
        "# transforms function provided above.                                       #\n",
        "#############################################################################\n",
        "\n",
        "small_dataset_config = dict(\n",
        "    data_per_class=10,\n",
        "    num_per_class=3,\n",
        "    class_type=[\"horizontal\", \"vertical\", \"none\"],\n",
        ")\n",
        "\n",
        "small_dataset = EdgeDetectionDataset(small_dataset_config, mode='train', transform=transforms)\n",
        "\n",
        "#############################################################################\n",
        "#                             END OF YOUR CODE                              #\n",
        "#############################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-T6tY_2fRNb"
      },
      "source": [
        "In this notebook, we will use pytorch dataloader to load the dataset. We will use `torch.utils.data.DataLoader` to load the dataset. `DataLoader` takes two arguments: `dataset` and `batch_size`. `dataset` is the dataset that we want to load. Note that `batch_size` is one of important hyperparameters. We will use `batch_size=32` for this problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vK8us1MAfRNb"
      },
      "outputs": [],
      "source": [
        "small_dataset_loader = None\n",
        "\n",
        "#############################################################################\n",
        "# TODO: Implement dataloader                                                #\n",
        "# Hint: You should flag shuffle = True for training data loader             #\n",
        "# This flag makes huge difference in training                               #\n",
        "#############################################################################\n",
        "\n",
        "small_dataset_loader = DataLoader(small_dataset, batch_size=32)\n",
        "\n",
        "#############################################################################\n",
        "#                             END OF YOUR CODE                              #\n",
        "#############################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99NdgqqbfRNb"
      },
      "source": [
        "### Model Architecture\n",
        "\n",
        "MLP has two hidden layer with 10 hidden units and 10 hidden units. The input size is 28x28=784 and the output size is 3. We use ReLU as the activation function. We use cross entropy loss as the loss function.\n",
        "\n",
        "MLP architecture: FC(784, 10) -> ReLU -> FC(10, 10) -> ReLU -> FC(10, 3)\n",
        "\n",
        "CNN has two convolutional layers followed by global average pooling and one fully connected layer. Both convolutional layers have 3 filters whose kernel size is 7. We use ReLU as the activation function. We use cross entropy loss as the loss function.\n",
        "\n",
        "CNN arhitecture is as follows: CONV - RELU - MAXPOOL - CONV - RELU - MAXPOOL - FC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msySKOb_fRNb"
      },
      "source": [
        "### Fitting on Small Dataset\n",
        "\n",
        "Now let's train the model on the small dataset. The final tranining loss should be around 100% for both models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "c0ruzHnCfRNc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Model has 606 parameters\n",
            "MLP Model has 39793 parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 500/500 [00:14<00:00, 34.82it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Acc: 100.0, MLP Acc: 100.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "set_seed(seed)\n",
        "\n",
        "lr = 1e-2\n",
        "num_epochs = 500\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "cnn_model = SimpleCNN(kernel_size=7)\n",
        "cnn_model.to(device)\n",
        "untrained_cnn_model = deepcopy(cnn_model)\n",
        "\n",
        "mlp_model = ThreeLayerMLP(hidden_dims=[50, 10])\n",
        "mlp_model.to(device)\n",
        "\n",
        "mlp_optimizer = optim.SGD(mlp_model.parameters(), lr=lr, momentum=0.9)\n",
        "cnn_optimizer = optim.SGD(cnn_model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "print(\"CNN Model has {} parameters\".format(count_parameters(cnn_model, only_trainable=True)))\n",
        "print(\"MLP Model has {} parameters\".format(count_parameters(mlp_model, only_trainable=True)))\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    train_one_epoch(cnn_model, cnn_optimizer, criterion, small_dataset_loader, device, epoch, verbose=False)\n",
        "    train_one_epoch(mlp_model, mlp_optimizer, criterion, small_dataset_loader, device, epoch, verbose=False)\n",
        "\n",
        "_, cnn_acc, _ = evaluate(cnn_model, criterion, small_dataset_loader, device, verbose=False)\n",
        "_, mlp_acc, _ = evaluate(mlp_model, criterion, small_dataset_loader, device, verbose=False)\n",
        "\n",
        "print(\"CNN Acc: {}, MLP Acc: {}\".format(cnn_acc, mlp_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHUkBSw6fRNc"
      },
      "source": [
        "We checked that both models can overfit the small dataset. This is one of the most important sanity check. If the model cannot overfit the small dataset, the model is not powerful enough to learn the dataset. In this case, we need to increase the size of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71sdjo6CfRNc"
      },
      "source": [
        "### Visualize Learned Filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ASl3VJ2zfRNc"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x100 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x100 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cnn_kernel = cnn_model.conv1.weight.data.clone().cpu()\n",
        "untrained_kernel = untrained_cnn_model.conv1.weight.data.clone().cpu()\n",
        "\n",
        "vis_kernel(cnn_kernel, ch=0, allkernels=False, title='Trained CNN Kernel')\n",
        "vis_kernel(untrained_kernel, ch=0, allkernels=False, title='Untrained CNN Kernel')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-hADVAffRNc"
      },
      "source": [
        "### Question\n",
        "\n",
        "**Can you find any interesting patterns in the learned filters?** Answer this question in your submission of the written assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KecY3INSfRNc"
      },
      "source": [
        "## Q2. Sweeping the Number of Training Images\n",
        "\n",
        "We understood the given task and checked that both models had enough expressive power. We will compare the performance of MLP and CNN by changing the number of data per class. We expect that the model with proper inductive biases on this task will fit with **fewer training examples**. And let's see which one has inductive biases. In this problem, we will use the same dataset for both models. We sweep the number of training images from 10 to 500. The validation set will be the same for all the experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "N5AldjOAfRNc"
      },
      "outputs": [],
      "source": [
        "set_seed(seed)\n",
        "\n",
        "train_loader_dict = dict()\n",
        "num_images_list = [10, 30, 50, 100]\n",
        "valid_loader = None\n",
        "\n",
        "transforms = T.Compose([T.ToTensor()])\n",
        "train_batch_size = 10\n",
        "valid_batch_size = 256\n",
        "#############################################################################\n",
        "# TODO: Implement train_loader_dict for each number of training images.     #\n",
        "# Key: The number of training images (5, 10, 30, 50, and 100)               #\n",
        "# Value: The corresponding dataloader                                       #\n",
        "# The validation set size is 50 images per class                            #\n",
        "#############################################################################\n",
        "\n",
        "data_config = dict(\n",
        "    data_per_class=10,\n",
        "    num_classes=3,\n",
        "    class_type=[\"horizontal\", \"vertical\", \"none\"],\n",
        ")\n",
        "\n",
        "for num_image in num_images_list:\n",
        "    data_config['data_per_class'] = num_image\n",
        "    tmp_dataset_train = EdgeDetectionDataset(data_config, mode='train')\n",
        "    train_loader_dict[num_image] = DataLoader(tmp_dataset_train, batch_size=train_batch_size)\n",
        "\n",
        "#############################################################################\n",
        "#                             END OF YOUR CODE                              #\n",
        "#############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4b3wTmmjfRNc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with 10 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/30 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m cnn_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(cnn_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs)):\n\u001b[1;32m---> 26\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnn_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     train_one_epoch(mlp_model, mlp_optimizer, criterion, train_loader, device, epoch, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m cnn_kernel_dict[num_image] \u001b[38;5;241m=\u001b[39m deepcopy(cnn_model\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach())\n",
            "Cell \u001b[1;32mIn[4], line 531\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, criterion, train_loader, device, epoch, log_interval, verbose)\u001b[0m\n\u001b[0;32m    528\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    529\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 531\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m    532\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    533\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32mc:\\Users\\22020\\.conda\\envs\\cs182hw1\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\22020\\.conda\\envs\\cs182hw1\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\22020\\.conda\\envs\\cs182hw1\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\22020\\.conda\\envs\\cs182hw1\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:317\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\22020\\.conda\\envs\\cs182hw1\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\22020\\.conda\\envs\\cs182hw1\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\22020\\.conda\\envs\\cs182hw1\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:192\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[0;32m    189\u001b[0m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    190\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m--> 192\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
            "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>"
          ]
        }
      ],
      "source": [
        "lr = 1e-2\n",
        "num_epochs = 30\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "cnn_acc_list = list()\n",
        "mlp_acc_list = list()\n",
        "\n",
        "cnn_kernel_dict = dict()\n",
        "untrained_cnn_kernel_dict = dict()\n",
        "\n",
        "for num_image, train_loader in train_loader_dict.items():\n",
        "    print(\"Training with {} images\".format(num_image))\n",
        "    set_seed(seed)\n",
        "    cnn_model = SimpleCNN(kernel_size=7)\n",
        "    untrained_cnn_model = deepcopy(cnn_model)\n",
        "    cnn_model.to(device)\n",
        "\n",
        "    mlp_model = ThreeLayerMLP(hidden_dims=[50, 10])\n",
        "    mlp_model.to(device)\n",
        "\n",
        "    mlp_optimizer = optim.SGD(mlp_model.parameters(), lr=lr, momentum=0.9)\n",
        "    cnn_optimizer = optim.SGD(cnn_model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        train_one_epoch(cnn_model, cnn_optimizer, criterion, train_loader, device, epoch, verbose=False)\n",
        "        train_one_epoch(mlp_model, mlp_optimizer, criterion, train_loader, device, epoch, verbose=False)\n",
        "\n",
        "    cnn_kernel_dict[num_image] = deepcopy(cnn_model.conv1.weight.cpu().detach())\n",
        "    untrained_cnn_kernel_dict[num_image] = deepcopy(untrained_cnn_model.conv1.weight.cpu().detach())\n",
        "\n",
        "    _, cnn_valid_acc, _ = evaluate(cnn_model, criterion, valid_loader, device, verbose=False)\n",
        "    _, mlp_valid_acc, _ = evaluate(mlp_model, criterion, valid_loader, device, verbose=False)\n",
        "\n",
        "    print(\"CNN Acc: {}, MLP Acc: {}\".format(cnn_valid_acc, mlp_valid_acc))\n",
        "    cnn_acc_list.append(cnn_valid_acc)\n",
        "    mlp_acc_list.append(mlp_valid_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h6JiSJ8fRNd"
      },
      "outputs": [],
      "source": [
        "## Plot the validation accuracy\n",
        "plt.clf()\n",
        "fig, ax = plt.subplots(1, 1, figsize=(3, 3), dpi=200)\n",
        "ax.plot(num_images_list, cnn_acc_list, marker='o', label='CNN')\n",
        "ax.plot(num_images_list, mlp_acc_list, marker='o', label='MLP')\n",
        "ax.set_xlabel('# of Training Images per Class')\n",
        "ax.set_ylabel('Validation Accuracy (%)')\n",
        "ax.legend()\n",
        "ax.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMC2EUiKfRNd"
      },
      "source": [
        "OK, in most cases, CNN looks like it is performing better than MLP. So can we conclude that CNN has the inductive biases of locality and translational invariance? Not yet. We need to conduct a series of other experiments to show that CNN has such inductive biases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5FYT3VJfRNd"
      },
      "source": [
        "Seemingly, the experiment result is odd. First, the performance of the low data regime ```num_train_images_per_class=10``` is very bad, considering the task is straightforward. Second, we observe that the performance of MLP is better than CNN at some point. At least, CNN should be much better even in a small data regime if it is translational equivariant. How do we debug the model? We will study how to debug the model in the following problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpq_lUkxfRNd"
      },
      "source": [
        "Here are some checklists that you can do to debug the problem.\n",
        "\n",
        "1. Did you check the dataset? For example, is the dataset balanced? Is the dataset noisy? Is the dataset too small?\n",
        "2. Did you check the model architecture? For example, is the model architecture powerful enough to learn the dataset? Is the model architecture too complex? Is the model architecture too simple?\n",
        "3. Did you check the model initialization? For example, is the model initialized properly? Is the model initialized randomly? Is the model initialized with the pre-trained weights?\n",
        "4. Did you check that the model is trained correctly? For example, does the kernel look like an edge detector? What would be the performance of CNN if kernels were initialized with edge detectors?\n",
        "5. Did you check the training procedure? For example, is the training procedure correct? Is the training procedure stable? Is the training procedure too slow?\n",
        "6. Did you optimize the hyperparameters? For example, learning rate, batch size, and the number of epochs.\n",
        "\n",
        "Note that we already checked the dataset, initialization, and model architecture. But we didn't check the step after 3. Let's step 4 first. We will first see what the learned weights look like, initialize the kernels with edge detectors, and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mozw0pMvfRNd"
      },
      "outputs": [],
      "source": [
        "for num_image, cnn_kernel in cnn_kernel_dict.items():\n",
        "    untrained_kernel = untrained_cnn_kernel_dict[num_image]\n",
        "    vis_kernel(cnn_kernel, ch=0, allkernels=False, title='Trained Kernel - data: {}'.format(num_image))\n",
        "    vis_kernel(untrained_kernel, ch=0, allkernels=False, title='Untrained Kernel - data: {}'.format(num_image))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI65xMKGfRNd"
      },
      "source": [
        "#### Question\n",
        "\n",
        "**Compare the learned kernels, untrainable kernels, and edge-detector kernels. What do you observe?** Answer this question in your submission of the written assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3x7J40mfRNe"
      },
      "source": [
        "### Injecting Inductive Bias: Initialize Kernels with Edge Detectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0EwMo0cfRNe"
      },
      "outputs": [],
      "source": [
        "lr = 1e-2\n",
        "num_epochs = 30\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "edge_init_cnn_acc_list = list()\n",
        "\n",
        "for num_image, train_loader in train_loader_dict.items():\n",
        "    print(\"Training with {} images\".format(num_image))\n",
        "    cnn_model = SimpleCNN(kernel_size=7)\n",
        "    init_conv_kernel_with_edge_detector(cnn_model)\n",
        "    freeze_conv_layer(cnn_model)\n",
        "    untrained_cnn_model = deepcopy(cnn_model)\n",
        "    cnn_model.to(device)\n",
        "\n",
        "    cnn_optimizer = optim.SGD(cnn_model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    # logging how training and validation accuracy changes\n",
        "    edge_init_cnn_valid_acc_list = []\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        cnn_train_loss, cnn_train_acc = train_one_epoch(cnn_model, cnn_optimizer, criterion, train_loader, device, epoch, verbose=False)\n",
        "\n",
        "    _, cnn_valid_acc, _ = evaluate(cnn_model, criterion, valid_loader, device, verbose=False)\n",
        "    print(\"CNN Acc: {}\".format(cnn_valid_acc))\n",
        "    edge_init_cnn_acc_list.append(cnn_valid_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LdbKDnXfRNe"
      },
      "outputs": [],
      "source": [
        "## Plot the validation accuracy\n",
        "plt.clf()\n",
        "fig, ax = plt.subplots(1, 1, figsize=(3.5, 3.5), dpi=200)\n",
        "plt.plot(num_images_list, cnn_acc_list, marker='o', label='Randomly Initialized')\n",
        "plt.plot(num_images_list, edge_init_cnn_acc_list, marker='o', label='Edge Initialized')\n",
        "ax.set_xlabel('# of Training Images per Class')\n",
        "ax.set_ylabel('Validation Accuracy (%)')\n",
        "ax.legend()\n",
        "ax.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6bWs2bAfRNe"
      },
      "source": [
        "### Question\n",
        "\n",
        "We freeze the convolutional layer and train only final layer (classifier) in this experiment. For a high data regime, the performance of CNN initialized with edge detectors is worse than CNN initialized with random weights. **Why do you think this happens?** Answer this question in your submission of the written assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_pzbR7xfRNe"
      },
      "source": [
        "## Q3. Checking the Training Procedure\n",
        "\n",
        "Checking the training procedure is very important. We must log at least training loss, training accuracy, validation loss, and validation accuracy. Let's log such training signals and find out what is going on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXqR0s7KfRNe"
      },
      "outputs": [],
      "source": [
        "def train_model(model, optimizer, num_epochs, train_loader, valid_loader):\n",
        "    criterion=nn.CrossEntropyLoss()\n",
        "    untrained_model = deepcopy(model)\n",
        "    train_acc_list, valid_acc_list, train_loss_list, valid_loss_list = [], [], [] ,[]\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        train_loss, train_acc = train_one_epoch(model, optimizer, criterion, train_loader, device, epoch, verbose=False)\n",
        "        valid_loss, valid_acc, confusion_matrix = evaluate(model, criterion, valid_loader, device, verbose=False)\n",
        "        train_acc_list.append(train_acc)\n",
        "        valid_acc_list.append(valid_acc)\n",
        "        train_loss_list.append(train_loss)\n",
        "        valid_loss_list.append(valid_loss)\n",
        "\n",
        "    return {\"final_valid_acc\": valid_acc_list[-1], \"train_acc\": train_acc_list, \"valid_acc\": valid_acc_list,\n",
        "            \"train_loss\": train_loss_list, \"valid_loss\": valid_loss_list,\n",
        "            \"confusion_matrix\": confusion_matrix}\n",
        "\n",
        "\n",
        "lr = 1e-2\n",
        "num_epochs = 30\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "cnn_acc_list = list()\n",
        "mlp_acc_list = list()\n",
        "\n",
        "cnn_kernel_dict = dict()\n",
        "untrained_cnn_kernel_dict = dict()\n",
        "\n",
        "for num_image, train_loader in train_loader_dict.items():\n",
        "    print(\"Training with {} images\".format(num_image))\n",
        "    set_seed(seed)\n",
        "    cnn_model = SimpleCNN(kernel_size=7)\n",
        "    untrained_cnn_model = deepcopy(cnn_model)\n",
        "    cnn_model.to(device)\n",
        "\n",
        "    mlp_model = ThreeLayerMLP(hidden_dims=[50, 10])\n",
        "    mlp_model.to(device)\n",
        "\n",
        "    mlp_optimizer = optim.SGD(mlp_model.parameters(), lr=lr, momentum=0.9)\n",
        "    cnn_optimizer = optim.SGD(cnn_model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    mlp_results = train_model(mlp_model, mlp_optimizer, num_epochs, train_loader, valid_loader)\n",
        "    cnn_results = train_model(cnn_model, cnn_optimizer, num_epochs, train_loader, valid_loader)\n",
        "\n",
        "    vis_training_curve(cnn_results[\"train_loss\"], cnn_results[\"train_acc\"], mlp_results[\"train_loss\"], mlp_results[\"train_acc\"])\n",
        "    vis_validation_curve(cnn_results[\"valid_loss\"], cnn_results[\"valid_acc\"], mlp_results[\"valid_loss\"], mlp_results[\"valid_acc\"])\n",
        "\n",
        "    print(\"CNN Acc: {}, MLP Acc: {}\".format(cnn_results[\"final_valid_acc\"], mlp_results[\"final_valid_acc\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQvCsj0ZfRNe"
      },
      "source": [
        "What is going on here? Validation loss and validation accuracy are not flat at the end. It means that the model is not converged. We need to train the model more. Let's train the model with the higher number of epochs. Increase the number of epochs until the validation loss and accuracy are flat.\n",
        "\n",
        "#### Question\n",
        "\n",
        "**List every epochs that you trained the model.** Final accuracy of CNN should be at least 95% for 30 images per class. Answer this question in your submission of the written assignment.\n",
        "\n",
        "#### Question\n",
        "\n",
        "**Check the learned kernels. What do you observe?** Answer this question in your submission of the written assignment.\n",
        "\n",
        "#### Question (Optional)\n",
        "\n",
        "You might find that with the high number of epochs, validation loss of MLP is increasing while validation accuracy increasing.  **How can we interpret this?** Answer this question in your submission of the written assignment.\n",
        "\n",
        "(Hint: you may find papers that discuss calibrations related to this question (e.g., [paper](https://arxiv.org/pdf/1706.04599.pdf))\n",
        "\n",
        "#### Question (Optional)\n",
        "\n",
        "Do hyperparameter tuning. **And list the best hyperparameter setting that you found and report the final accuracy of CNN and MLP.** Answer this question in your submission of the written assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qbt7NCcmfRNe"
      },
      "outputs": [],
      "source": [
        "#############################################################################\n",
        "# TODO: Try other num_epochs. Final accuracy of CNN should be at around     #\n",
        "# 95-99% for 30 images per class.                                           #\n",
        "#############################################################################\n",
        "#############################################################################\n",
        "#                             END OF YOUR CODE                              #\n",
        "#############################################################################\n",
        "lr = 1e-2\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "cnn_acc_list = list()\n",
        "mlp_acc_list = list()\n",
        "\n",
        "cnn_kernel_dict = dict()\n",
        "untrained_cnn_kernel_dict = dict()\n",
        "\n",
        "for num_image, train_loader in train_loader_dict.items():\n",
        "    print(\"Training with {} images\".format(num_image))\n",
        "    set_seed(seed)\n",
        "    cnn_model = SimpleCNN(kernel_size=7)\n",
        "    untrained_cnn_model = deepcopy(cnn_model)\n",
        "    cnn_model.to(device)\n",
        "\n",
        "    mlp_model = ThreeLayerMLP(hidden_dims=[50, 10])\n",
        "    mlp_model.to(device)\n",
        "\n",
        "    mlp_optimizer = optim.SGD(mlp_model.parameters(), lr=lr, momentum=0.9)\n",
        "    cnn_optimizer = optim.SGD(cnn_model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    mlp_results = train_model(mlp_model, mlp_optimizer, num_epochs, train_loader, valid_loader)\n",
        "    cnn_results = train_model(cnn_model, cnn_optimizer, num_epochs, train_loader, valid_loader)\n",
        "\n",
        "    vis_training_curve(cnn_results[\"train_loss\"], cnn_results[\"train_acc\"], mlp_results[\"train_loss\"], mlp_results[\"train_acc\"])\n",
        "    vis_validation_curve(cnn_results[\"valid_loss\"], cnn_results[\"valid_acc\"], mlp_results[\"valid_loss\"], mlp_results[\"valid_acc\"])\n",
        "\n",
        "    cnn_kernel_dict[num_image] = deepcopy(cnn_model.conv1.weight.data.detach().cpu())\n",
        "    untrained_cnn_kernel_dict[num_image] = deepcopy(untrained_cnn_model.conv1.weight.data.detach().cpu())\n",
        "\n",
        "    print(\"CNN Acc: {}, MLP Acc: {}\".format(cnn_results[\"final_valid_acc\"], mlp_results[\"final_valid_acc\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yQGgyJdfRNe"
      },
      "outputs": [],
      "source": [
        "for num_image, cnn_kernel in cnn_kernel_dict.items():\n",
        "    untrained_kernel = untrained_cnn_kernel_dict[num_image]\n",
        "    vis_kernel(cnn_kernel, ch=0, allkernels=False, title='Trained CNN Kernel {}'.format(num_image))\n",
        "    vis_kernel(untrained_kernel, ch=0, allkernels=False, title='Untrained CNN Kernel {}'.format(num_image))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtKu3HnBfRNe"
      },
      "source": [
        "#### Question\n",
        "\n",
        "**How much more data is needed for MLP to get a competitive performance with CNN?** Answer this question in your submission of the written assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIFf7FlffRNf"
      },
      "source": [
        "## Q4. Domain Shift between Training and Validation Set\n",
        "\n",
        "In this problem, we will see how the model performance changes when the domain of the training set and that of the validation set are different. We will generate training set images with edges that locate only half of the image and validation set images with edges that locate only the other half of the image. Let's repeat the same experiment as the previous problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGG2xDmSfRNf"
      },
      "outputs": [],
      "source": [
        "set_seed(seed)\n",
        "train_loader_dict = dict()\n",
        "num_train_images_list = [10, 30, 50, 100]\n",
        "possible_edge_location_ratio = 0.5\n",
        "valid_loader = None\n",
        "\n",
        "transforms = T.Compose([T.ToTensor()])\n",
        "batch_size = 10\n",
        "\n",
        "for num_image in num_train_images_list:\n",
        "    train_dataset_config = dict(\n",
        "        data_per_class=num_image,\n",
        "        possible_edge_location_ratio=possible_edge_location_ratio,\n",
        "    )\n",
        "    train_dataset = EdgeDetectionDataset(train_dataset_config, 'train', transform=transforms)\n",
        "    train_loader_dict[num_image] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "valid_dataset_config = dict(\n",
        "    data_per_class=50,\n",
        "    possible_edge_location_ratio=possible_edge_location_ratio,\n",
        ")\n",
        "valid_dataset = EdgeDetectionDataset(valid_dataset_config, 'valid', transform=transforms)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caOnxNaJfRNf"
      },
      "outputs": [],
      "source": [
        "lr = 1e-2\n",
        "num_epochs = 300\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "cnn_acc_list = list()\n",
        "mlp_acc_list = list()\n",
        "\n",
        "cnn_kernel_dict = dict()\n",
        "untrained_cnn_kernel_dict = dict()\n",
        "\n",
        "cnn_confusion_matrix_dict = dict()\n",
        "mlp_confusion_matrix_dict = dict()\n",
        "\n",
        "for num_image, train_loader in train_loader_dict.items():\n",
        "    print(\"Training with {} images\".format(num_image))\n",
        "    set_seed(seed)\n",
        "    cnn_model = SimpleCNN(kernel_size=7)\n",
        "    untrained_cnn_model = deepcopy(cnn_model)\n",
        "    cnn_model.to(device)\n",
        "\n",
        "    mlp_model = ThreeLayerMLP(hidden_dims=[50, 10])\n",
        "    mlp_model.to(device)\n",
        "\n",
        "    mlp_optimizer = optim.SGD(mlp_model.parameters(), lr=lr, momentum=0.9)\n",
        "    cnn_optimizer = optim.SGD(cnn_model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    mlp_results = train_model(mlp_model, mlp_optimizer, num_epochs, train_loader, valid_loader)\n",
        "    cnn_results = train_model(cnn_model, cnn_optimizer, num_epochs, train_loader, valid_loader)\n",
        "\n",
        "    vis_training_curve(cnn_results[\"train_loss\"], cnn_results[\"train_acc\"], mlp_results[\"train_loss\"], mlp_results[\"train_acc\"])\n",
        "    vis_validation_curve(cnn_results[\"valid_loss\"], cnn_results[\"valid_acc\"], mlp_results[\"valid_loss\"], mlp_results[\"valid_acc\"])\n",
        "\n",
        "    cnn_kernel_dict[num_image] = deepcopy(cnn_model.conv1.weight.detach().cpu())\n",
        "    untrained_cnn_kernel_dict[num_image] = deepcopy(untrained_cnn_model.conv1.weight.detach().cpu())\n",
        "\n",
        "    cnn_confusion_matrix_dict[num_image] = cnn_results[\"confusion_matrix\"]\n",
        "    mlp_confusion_matrix_dict[num_image] = mlp_results[\"confusion_matrix\"]\n",
        "\n",
        "    print(\"CNN Acc: {}, MLP Acc: {}\".format(cnn_results[\"final_valid_acc\"], mlp_results[\"final_valid_acc\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x0tC41IfRNf"
      },
      "outputs": [],
      "source": [
        "for num_image, cnn_kernel in cnn_kernel_dict.items():\n",
        "    untrained_kernel = untrained_cnn_kernel_dict[num_image]\n",
        "    vis_kernel(cnn_kernel, ch=0, allkernels=False, title='Trained CNN Kernel Data={}'.format(num_image))\n",
        "    vis_kernel(untrained_kernel, ch=0, allkernels=False, title='Untrained CNN Kernel Data={}'.format(num_image))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpwH_D8cfRNf"
      },
      "source": [
        "In this example, you will see that both CNN and MLP performance are worse than those in the previous question. If two models learn how to extract edges, they should be able to classify the images with edges even though the edges locate in the other half of the images. However, both models suffer from performance degration (especially for MLP). What would be the problem? To investigate this, let's first look at the confusion matrices for both models  [link](https://en.wikipedia.org/wiki/Confusion_matrix)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvNbN5_jfRNf"
      },
      "outputs": [],
      "source": [
        "## Plot the confusion matrix\n",
        "for num_image, cnn_confusion_matrix in cnn_confusion_matrix_dict.items():\n",
        "    mlp_confusion_matrix = mlp_confusion_matrix_dict[num_image]\n",
        "    vis_confusion_matrix(cnn_confusion_matrix, ['horizontal', 'vertical', 'none'], 'CNN-{}-images'.format(num_image))\n",
        "    vis_confusion_matrix(mlp_confusion_matrix, ['horizontal', 'vertical', 'none'], 'MLP-{}-images'.format(num_image))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PwtdNiqfRNf"
      },
      "source": [
        "#### Question\n",
        "\n",
        "**Why do you think the confusion matrix looks like this?** Answer this question in your submission of the written assignment.\n",
        "\n",
        "(Hint: Visualize some of the images in the training and validation set. And we are using kernel_size=7, which is large relative to the image size.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L36l7mWWfRNf"
      },
      "source": [
        "We can do better than this. We didn't explore hyperparameter space yet. Let's search hyperparameters that can generalize well to the validation set. We will change the learning rate, the number of epochs, and kernel size for CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7tle6ihfRNf"
      },
      "outputs": [],
      "source": [
        "#############################################################################\n",
        "# TODO: Try other num_epochs, lr, kernel_size. The validation accuracy      #\n",
        "# should achieve around 97-100% for 10 images per class.                    #\n",
        "#############################################################################\n",
        "#############################################################################\n",
        "#                             END OF YOUR CODE                              #\n",
        "#############################################################################\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "cnn_valid_acc_list = list()\n",
        "\n",
        "cnn_kernel_dict = dict()\n",
        "\n",
        "cnn_confusion_matrix_dict = dict()\n",
        "\n",
        "for num_image, train_loader in train_loader_dict.items():\n",
        "    print(\"Training with {} images\".format(num_image))\n",
        "    set_seed(seed)\n",
        "    cnn_model = SimpleCNN(kernel_size=kernel_size)\n",
        "    untrained_cnn_model = deepcopy(cnn_model)\n",
        "    cnn_model.to(device)\n",
        "\n",
        "    cnn_optimizer = optim.SGD(cnn_model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    cnn_results = train_model(cnn_model, cnn_optimizer, num_epochs, train_loader, valid_loader)\n",
        "\n",
        "    vis_training_curve(cnn_results[\"train_loss\"], cnn_results[\"train_acc\"], None, None)\n",
        "    vis_validation_curve(cnn_results[\"valid_loss\"], cnn_results[\"valid_acc\"], None, None)\n",
        "\n",
        "    print(\"CNN Acc: {}\".format(cnn_results[\"final_valid_acc\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfU_iYipfRNg"
      },
      "source": [
        "#### Question\n",
        "\n",
        "**Why do you think MLP fails to learn the task while CNN can learn the task?** Answer this question in your submission of the written assignment.\n",
        "\n",
        "(Hint: Think about the model architecture.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJW-oM8CfRNg"
      },
      "source": [
        "## Q5. When is CNN Worse than MLP?\n",
        "\n",
        "In this problem, we will see that CNN is not always better than MLP in the image domain. Using CNN assumes that the data has locally correlated, whatever data looks. We can manually 'whiten' or remove such local correlation simply by applying random permutation to the images. A random permutation matrix is a matrix that has the same number of rows and columns. Each row and column has the same number of 1s. The rest of the elements are 0s. For example, the following is a random permutation matrix.\n",
        "\n",
        "```\n",
        "[[0, 1, 0, 0],\n",
        " [0, 0, 0, 1],\n",
        " [1, 0, 0, 0],\n",
        " [0, 0, 1, 0]]\n",
        "```\n",
        "\n",
        "This matrix randomly reorders the elements of the vector. For example, if we apply this matrix to the vector `[1, 2, 3, 4]`, we will get `[2, 4, 1, 3]`. If we apply this matrix to the image, we will get the image with the same content, but the pixels are randomly shuffled. One property of the random permutation matrix is that it is invertible. It means that we can recover the original image by simply applying the inverse matrix to the shuffled image. From the information-theoretical perspective, the random permutation matrix preserves the mutual information of the image and the label.\n",
        "\n",
        "We will repeat the same experiment as the previous problem. Visualize the dataset first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpJXVvFSfRNg"
      },
      "outputs": [],
      "source": [
        "set_seed(seed)\n",
        "visual_domain_config = None\n",
        "use_permutation = True\n",
        "\n",
        "permutater = np.arange(28 * 28,  dtype=np.int32)\n",
        "np.random.shuffle(permutater)\n",
        "unpermutater = np.argsort(permutater)\n",
        "\n",
        "visual_dataset = None\n",
        "\n",
        "transforms = T.Compose([T.ToTensor()])\n",
        "\n",
        "visual_domain_config = dict(\n",
        "    data_per_class=10,\n",
        "    use_permutation=True,\n",
        "    permutater=permutater,\n",
        "    unpermutater=unpermutater,\n",
        ")\n",
        "visual_dataset = EdgeDetectionDataset(visual_domain_config, mode='train', transform=transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZlup1KVfRNg"
      },
      "outputs": [],
      "source": [
        "## Visualize the images\n",
        "unpermutator = visual_dataset.get_unpermutater()\n",
        "print('Dataset Image before permutation')\n",
        "vis_unpermuted_dataset(visual_dataset, num_classes=3, num_show_per_class=10, unpermutator=unpermutator)\n",
        "\n",
        "print('Dataset Image after permutation')\n",
        "vis_dataset(visual_dataset, num_classes=3, num_show_per_class=10)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRzBU-dmfRNg"
      },
      "source": [
        "Now let's train CNN and MLP on the permuted dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNMCIS9JfRNg"
      },
      "outputs": [],
      "source": [
        "set_seed(seed)\n",
        "\n",
        "train_loader_dict = dict()\n",
        "num_train_images_list = [10, 30, 50, 100]\n",
        "use_permutation = True\n",
        "valid_loader = None\n",
        "\n",
        "permutater = np.arange(28 * 28,  dtype=np.int32)\n",
        "np.random.shuffle(permutater)\n",
        "unpermutater = np.argsort(permutater)\n",
        "\n",
        "transforms = T.Compose([T.ToTensor()])\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "for num_image in num_train_images_list:\n",
        "    train_dataset_config = dict(\n",
        "        data_per_class=num_image,\n",
        "        use_permutation=True,\n",
        "        permutater=permutater,\n",
        "        unpermutater=unpermutater,\n",
        "    )\n",
        "    train_dataset = EdgeDetectionDataset(train_dataset_config, 'train', transform=transforms)\n",
        "    train_loader_dict[num_image] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "valid_dataset_config = dict(\n",
        "    data_per_class=50,\n",
        "    use_permutation=True,\n",
        "    permutater=permutater,\n",
        "    unpermutater=unpermutater,\n",
        ")\n",
        "valid_dataset = EdgeDetectionDataset(valid_dataset_config, 'valid', transform=transforms)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDtXqFzGfRNg"
      },
      "outputs": [],
      "source": [
        "lr = 1e-2\n",
        "num_epochs = 300\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "cnn_kernel_dict = dict()\n",
        "untrained_cnn_kernel_dict = dict()\n",
        "\n",
        "for num_image, train_loader in train_loader_dict.items():\n",
        "    print(\"Training with {} images\".format(num_image))\n",
        "    set_seed(seed)\n",
        "    cnn_model = SimpleCNN(kernel_size=7)\n",
        "    untrained_cnn_model = deepcopy(cnn_model)\n",
        "    cnn_model.to(device)\n",
        "\n",
        "    mlp_model = ThreeLayerMLP(hidden_dims=[50, 10])\n",
        "    mlp_model.to(device)\n",
        "\n",
        "    mlp_optimizer = optim.SGD(mlp_model.parameters(), lr=lr, momentum=0.9)\n",
        "    cnn_optimizer = optim.SGD(cnn_model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    mlp_results = train_model(mlp_model, mlp_optimizer, num_epochs, train_loader, valid_loader)\n",
        "    cnn_results = train_model(cnn_model, cnn_optimizer, num_epochs, train_loader, valid_loader)\n",
        "\n",
        "    vis_training_curve(cnn_results[\"train_loss\"], cnn_results[\"train_acc\"], mlp_results[\"train_loss\"], mlp_results[\"train_acc\"])\n",
        "    vis_validation_curve(cnn_results[\"valid_loss\"], cnn_results[\"valid_acc\"], mlp_results[\"valid_loss\"], mlp_results[\"valid_acc\"])\n",
        "\n",
        "    cnn_kernel_dict[num_image] = cnn_model.conv1.weight.detach().cpu()\n",
        "    untrained_cnn_kernel_dict[num_image] = untrained_cnn_model.conv1.weight.detach().cpu()\n",
        "\n",
        "    print(\"CNN Acc: {}, MLP Acc: {}\".format(cnn_results[\"final_valid_acc\"], mlp_results[\"final_valid_acc\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAyBwfr-fRNg"
      },
      "source": [
        "#### Question\n",
        "\n",
        "**What do you observe? What is the reason that CNN is worse than MLP?** Answer this question in your submission of the written assignment.\n",
        "\n",
        "(Hint: Think about the model architecture.)\n",
        "\n",
        "#### Question\n",
        "\n",
        "**Assuming we are decreasing kernel size of CNN. Does the validation accuracy increase or decrease? Why?** Answer this question in your submission of the written assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdqsWCBvfRNg"
      },
      "source": [
        "Now let's visualize CNN's learned kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7u0UwgnfRNg"
      },
      "outputs": [],
      "source": [
        "for num_image, cnn_kernel in cnn_kernel_dict.items():\n",
        "    untrained_kernel = untrained_cnn_kernel_dict[num_image]\n",
        "    vis_kernel(cnn_kernel, ch=0, allkernels=False, title='Trained CNN Kernel Data={}'.format(num_image))\n",
        "    vis_kernel(untrained_kernel, ch=0, allkernels=False, title='Untrained CNN Kernel Data={}'.format(num_image))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOMAZ7T2fRNh"
      },
      "source": [
        "\n",
        "#### Question\n",
        "\n",
        "**How do the learned kernels look like? Explain why.** Answer this question in your submission of the written assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8LB02NYfRNh"
      },
      "source": [
        "From the above example, we can see that CNN is not always better than MLP. We have to think about the domain (or task) of the dataset and the model architecture to decide which model is better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdQWauLNfRNh"
      },
      "source": [
        "## Q6. Increasing the Number of Classes\n",
        "\n",
        "OK, can we conclude that CNN has the inductive bias that the model is translation invariant? Let's try other experiments. We make the task harder. In this problem, we increase the number of classes to 5. The new classes are 0 for horizontal edges, 1 for vertical edges, 2 for diagonal edges, 3 for vertical and horizontal, and 4 for nothing. Let's generate the dataset with 10 images per class and visualize the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nd8_YExfRNh"
      },
      "outputs": [],
      "source": [
        "set_seed(seed)\n",
        "visual_domain_config = None\n",
        "\n",
        "visual_dataset = None\n",
        "\n",
        "transforms = T.Compose([T.ToTensor()])\n",
        "visual_domain_config = dict(\n",
        "    data_per_class=10,\n",
        "    class_type=['horizontal', 'vertical', 'diagonal', 'both', 'none'],\n",
        "    num_classes=5,\n",
        ")\n",
        "\n",
        "visual_dataset = EdgeDetectionDataset(visual_domain_config, 'train', transform=transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF5MBTlzfRNh"
      },
      "source": [
        "Let's visualize the dataset first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKB6p2K7fRNh"
      },
      "outputs": [],
      "source": [
        "vis_dataset(visual_dataset, 5, 10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aePWqygDfRNh"
      },
      "source": [
        "Now let's make the new dataset. In this problem, we also see how the model performance changes as the number of images per class increases. Let's sweep the number of training images 10, 30, 50, and 100. The validation set will be the same (50) for all the cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7p-64oqfRNh"
      },
      "outputs": [],
      "source": [
        "set_seed(seed)\n",
        "\n",
        "train_dataset_config = None\n",
        "train_loader_dict = dict()\n",
        "num_train_images_list = [10, 30, 50, 100]\n",
        "valid_loader = None\n",
        "\n",
        "transforms = T.Compose([T.ToTensor()])\n",
        "batch_size = 10\n",
        "class_type = ['horizontal', 'vertical', 'diagonal', 'both', 'none']\n",
        "train_dataset_config = dict(\n",
        "    class_type=class_type,\n",
        "    num_classes=len(class_type),\n",
        ")\n",
        "for num_train_images in num_train_images_list:\n",
        "    train_dataset_config['data_per_class'] = num_train_images\n",
        "    train_dataset = EdgeDetectionDataset(train_dataset_config, 'train', transform=transforms)\n",
        "    train_loader_dict[num_train_images] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "valid_dataset_config = dict(\n",
        "    data_per_class=50,\n",
        "    class_type=['horizontal', 'vertical', 'diagonal', 'both', 'none'],\n",
        "    num_classes=len(class_type),\n",
        ")\n",
        "valid_dataset = EdgeDetectionDataset(valid_dataset_config, 'valid', transform=transforms)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aRt_awnfRNh"
      },
      "outputs": [],
      "source": [
        "lr = 1e-2\n",
        "num_epochs = 200\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "cnn_kernel_dict = dict()\n",
        "untrained_cnn_kernel_dict = dict()\n",
        "\n",
        "cnn_confusion_matrix_dict = dict()\n",
        "mlp_confusion_matrix_dict = dict()\n",
        "\n",
        "cnn_result_dict = dict()\n",
        "for num_image, train_loader in train_loader_dict.items():\n",
        "    print(\"Training with {} images\".format(num_image))\n",
        "    set_seed(seed)\n",
        "    cnn_model = SimpleCNN(kernel_size=7, num_classes=5)\n",
        "    untrained_cnn_model = deepcopy(cnn_model)\n",
        "    cnn_model.to(device)\n",
        "\n",
        "    mlp_model = ThreeLayerMLP(hidden_dims=[50, 10], num_classes=5)\n",
        "    mlp_model.to(device)\n",
        "\n",
        "    mlp_optimizer = optim.SGD(mlp_model.parameters(), lr=lr, momentum=0.9)\n",
        "    cnn_optimizer = optim.SGD(cnn_model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    mlp_results = train_model(mlp_model, mlp_optimizer, num_epochs, train_loader, valid_loader)\n",
        "    cnn_results = train_model(cnn_model, cnn_optimizer, num_epochs, train_loader, valid_loader)\n",
        "\n",
        "    vis_training_curve(cnn_results[\"train_loss\"], cnn_results[\"train_acc\"], mlp_results[\"train_loss\"], mlp_results[\"train_acc\"])\n",
        "    vis_validation_curve(cnn_results[\"valid_loss\"], cnn_results[\"valid_acc\"], mlp_results[\"valid_loss\"], mlp_results[\"valid_acc\"])\n",
        "\n",
        "    cnn_kernel_dict[num_image] = cnn_model.conv1.weight.detach().cpu()\n",
        "    untrained_cnn_kernel_dict[num_image] = untrained_cnn_model.conv1.weight.detach().cpu()\n",
        "\n",
        "    cnn_result_dict[num_image] = cnn_results\n",
        "    print(\"CNN Acc: {}, MLP Acc: {}\".format(cnn_results[\"final_valid_acc\"], mlp_results[\"final_valid_acc\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiVF8SsFfRNh"
      },
      "source": [
        "*We look at two types of pooling operations to downsample the image features:*\n",
        "\n",
        "1) Max pooling: The maximum pixel value of the batch is selected.\n",
        "2) Average pooling: The average value of all the pixels in the batch is selected.\n",
        "\n",
        "Up until this point, we have been using the first type of pooling operation (Max pooling). Let's train the same model but with the average pooling to compare these two types of operations!  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q4gkdkXfRNh"
      },
      "outputs": [],
      "source": [
        "lr = 1e-2\n",
        "num_epochs = 200\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "cnnavg_kernel_dict = dict()\n",
        "\n",
        "for num_image, train_loader in train_loader_dict.items():\n",
        "    print(\"Training with {} images\".format(num_image))\n",
        "    set_seed(seed)\n",
        "\n",
        "    cnnavg_model = SimpleCNN_avgpool(kernel_size=7, num_classes=5)\n",
        "    cnnavg_model.to(device)\n",
        "\n",
        "    cnnavg_optimizer = optim.SGD(cnnavg_model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    cnnavg_results = train_model(cnnavg_model, cnnavg_optimizer, num_epochs, train_loader, valid_loader)\n",
        "    cnn_results = cnn_result_dict[num_image] # load the results from the previous cell as we have already trained the maxpool model.\n",
        "\n",
        "    vis_training_curve(cnn_results[\"train_loss\"], cnn_results[\"train_acc\"], cnnavg_results[\"train_loss\"], cnnavg_results[\"train_acc\"], label=\"CNN-avgpool\")\n",
        "    vis_validation_curve(cnn_results[\"valid_loss\"], cnn_results[\"valid_acc\"], cnnavg_results[\"valid_loss\"], cnnavg_results[\"valid_acc\"], label=\"CNN-avgpool\")\n",
        "\n",
        "    cnnavg_kernel_dict[num_image] = cnnavg_model.conv1.weight.detach().cpu()\n",
        "\n",
        "    print(\"CNN-maxpool Acc: {}, CNN-avgpool Acc: {}\".format(cnn_results[\"final_valid_acc\"], cnnavg_results[\"final_valid_acc\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xM336hS8fRNh"
      },
      "outputs": [],
      "source": [
        "for num_image, cnn_kernel in cnn_kernel_dict.items():\n",
        "    untrained_kernel = untrained_cnn_kernel_dict[num_image]\n",
        "    cnnavg_kernel = cnnavg_kernel_dict[num_image]\n",
        "    vis_kernel(cnn_kernel, ch=0, allkernels=False, title='Trained CNN Kernel Maxpool Data={}'.format(num_image))\n",
        "    vis_kernel(cnnavg_kernel, ch=0, allkernels=False, title='Trained CNN Kernel Avgpool Data={}'.format(num_image))\n",
        "    vis_kernel(untrained_kernel, ch=0, allkernels=False, title='Untrained CNN Kernel Data={}'.format(num_image))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRK9EJCtfRNi"
      },
      "source": [
        "#### Question\n",
        "\n",
        "**Compare the performance of CNN with max pooling and average pooling. What are the advantages of each pooling method?** Answer this question in your submission of the written assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEe8DamEfRNi"
      },
      "source": [
        "\n",
        "## Q7. Wider/Deeper CNNs\n",
        "\n",
        "Can we further improve the performance by making the architecture deeper and wider? In this question, we focus on the dataset where there are only 30 images per class and try to push the performance of the CNNs further.\n",
        "\n",
        "The patterns that we have to detect are 5 but our kernels per layer (`num_filters` in the network definition above) are only 3. Intuitively, this is quite a suboptimal. Here, we will investigate the affect of increasing width and depth. Let's use the same dataset but we will use ```DeeperCNN``` and ```WiderCNN``` in ```cnn.py```. ```DeeperCNN``` has 2 times more layers than ```SimpleCNN``` and ```WiderCNN``` has 2 times more kernels per layer than ```SimpleCNN```. Let's train the models and visualize the validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyTaEqSCfRNi"
      },
      "outputs": [],
      "source": [
        "#############################################################################\n",
        "# TODO: Train DeeperCNN and tuning hyperparameters. Try other num_epochs,   #\n",
        "# lr, kernel_size. Also try a different optimizer (e.g., Adam)              #\n",
        "# The validation accuracy can reach above 98% for 30 images per class.      #\n",
        "#############################################################################\n",
        "#############################################################################\n",
        "#                             END OF YOUR CODE                              #\n",
        "#############################################################################\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_loader = train_loader_dict[30]\n",
        "set_seed(seed)\n",
        "deeper_cnn_model = DeeperCNN(kernel_size=kernel_size)\n",
        "untrained_deeper_cnn_model = deepcopy(deeper_cnn_model)\n",
        "deeper_cnn_model.to(device)\n",
        "\n",
        "deeper_cnn_optimizer = optim.SGD(deeper_cnn_model.parameters(), lr=lr, momentum=0.9)\n",
        "# deeper_cnn_optimizer = optim.Adam(deeper_cnn_model.parameters(), lr=lr)  # try me!\n",
        "\n",
        "deeper_cnn_results = train_model(deeper_cnn_model, deeper_cnn_optimizer, num_epochs, train_loader, valid_loader)\n",
        "\n",
        "vis_training_curve(deeper_cnn_results[\"train_loss\"], deeper_cnn_results[\"train_acc\"], None, None)\n",
        "vis_validation_curve(deeper_cnn_results[\"valid_loss\"], deeper_cnn_results[\"valid_acc\"], None, None)\n",
        "\n",
        "print(\"CNN Acc: {}\".format(deeper_cnn_results[\"final_valid_acc\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WJD44jZfRNi"
      },
      "outputs": [],
      "source": [
        "#############################################################################\n",
        "# TODO: Train DeeperCNN and tuning hyperparameters. Try other num_epochs,   #\n",
        "# lr, kernel_size. Also try a different optimizer (e.g., Adam)              #\n",
        "# The validation accuracy can reach above 98% for 30 images per class.      #\n",
        "#############################################################################\n",
        "#############################################################################\n",
        "#                             END OF YOUR CODE                              #\n",
        "#############################################################################\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_loader = train_loader_dict[30]\n",
        "set_seed(seed)\n",
        "wider_cnn_model = WiderCNN(kernel_size=kernel_size)\n",
        "untrained_wider_cnn_model = deepcopy(wider_cnn_model)\n",
        "wider_cnn_model.to(device)\n",
        "\n",
        "wider_cnn_optimizer = optim.SGD(wider_cnn_model.parameters(), lr=lr, momentum=0.9)\n",
        "# wider_cnn_optimizer = optim.Adam(wider_cnn_model.parameters(), lr=lr)  # try me!\n",
        "\n",
        "wider_cnn_results = train_model(wider_cnn_model, wider_cnn_optimizer, num_epochs, train_loader, valid_loader)\n",
        "\n",
        "vis_training_curve(wider_cnn_results[\"train_loss\"], wider_cnn_results[\"train_acc\"], None, None)\n",
        "vis_validation_curve(wider_cnn_results[\"valid_loss\"], wider_cnn_results[\"valid_acc\"], None, None)\n",
        "\n",
        "print(\"CNN Acc: {}\".format(wider_cnn_results[\"final_valid_acc\"]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "cs182hw1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
